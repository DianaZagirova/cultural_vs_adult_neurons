{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4222026-9f6c-4e3f-ba64-d7bbfe21797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import cooler\n",
    "import bioframe\n",
    "import cooltools\n",
    "from cooltools.lib.numutils import fill_diag\n",
    "from pybedtools import BedTool as pbt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import EngFormatter\n",
    "from itertools import chain\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8eb49edf-05fc-484a-a3fe-3ba2adcd6029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cultures_hic\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f3afba5-1b4a-4b1b-a56d-761367c79556",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromnames = ['chr1',\n",
    " 'chr2',\n",
    " 'chr3',\n",
    " 'chr4',\n",
    " 'chr5',\n",
    " 'chr6',\n",
    " 'chr7',\n",
    " 'chr8',\n",
    " 'chr9',\n",
    " 'chr10',\n",
    " 'chr11',\n",
    " 'chr12',\n",
    " 'chr13',\n",
    " 'chr14',\n",
    " 'chr15',\n",
    " 'chr16',\n",
    " 'chr17',\n",
    " 'chr18',\n",
    " 'chr19',\n",
    " 'chr20',\n",
    " 'chr21',\n",
    " 'chr22',\n",
    " 'chrX']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb437fa-5299-412f-befb-f51d3054d16a",
   "metadata": {},
   "source": [
    "# 1. Load maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9bde452-12d9-4bc3-b5f6-c37b0c4a8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_maps(path_to_maps, pattern = '.mcool', addtitional_pattern = None):\n",
    "    files = [f for f in listdir(path_to_maps) if pattern in f]\n",
    "    if addtitional_pattern:\n",
    "        files = [f for f in listdir(path_to_maps) if addtitional_pattern in f]\n",
    "    return files\n",
    "\n",
    "def get_chromosomes():\n",
    "    hg38_chromsizes = bioframe.fetch_chromsizes('hg38')\n",
    "    hg38_cens = bioframe.fetch_centromeres('hg38')\n",
    "    hg38_arms = bioframe.make_chromarms(hg38_chromsizes, hg38_cens)\n",
    "    return hg38_arms.set_index(\"chrom\").loc[chromnames].reset_index()\n",
    "\n",
    "def get_expected_map(clr, name_exp, binsize, save_path = None):\n",
    "    exp = cooltools.expected_cis(\n",
    "    clr,\n",
    "    view_df=hg38_arms,\n",
    "    nproc=18)\n",
    "    if save_path:\n",
    "        exp.to_pickle(f\"{save_path}/{name_exp}_{binsize}res.pickle\")\n",
    "    return exp\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def load_or_compute_expected_map(map_name, path_to_maps, path_to_maps_expected, hg38_arms, binsize=15_000, nproc=14):\n",
    "    \"\"\"\n",
    "    Load expected map from a pickle file or compute it if not available.\n",
    "\n",
    "    Parameters:\n",
    "    - map_name: str, name of the map file.\n",
    "    - path_to_maps: str, path to the directory containing map files.\n",
    "    - path_to_maps_expected: str, path to the directory for storing expected map pickles.\n",
    "    - hg38_arms: DataFrame, view dataframe for cooler.\n",
    "    - binsize: int, bin size for cooler.\n",
    "    - nproc: int, number of processes to use.\n",
    "\n",
    "    Returns:\n",
    "    - expected_per_chrArm: DataFrame, expected values per chromosome arm.\n",
    "    \"\"\"\n",
    "    map_prefix = map_name.split('.mcool')[0]\n",
    "    pickle_file = os.path.join(path_to_maps_expected, f'{map_prefix}_perChrArm.pickle')\n",
    "    \n",
    "    if os.path.exists(pickle_file):\n",
    "        logging.info(f\"Loading expected map from {pickle_file}\")\n",
    "        expected_per_chrArm = pd.read_pickle(pickle_file)\n",
    "    else:\n",
    "        logging.info(f\"Computing expected map for {map_name}\")\n",
    "        clr = cooler.Cooler(f'{path_to_maps}/{map_name}::/resolutions/{binsize}')\n",
    "        expected_per_chrArm = cooltools.expected_cis(clr, view_df=hg38_arms, nproc=nproc)\n",
    "        expected_per_chrArm.to_pickle(pickle_file)\n",
    "        logging.info(f\"Expected map saved to {pickle_file}\")\n",
    "    \n",
    "    return expected_per_chrArm\n",
    "\n",
    "def process_maps(files, path_to_maps, path_to_maps_expected, hg38_arms, binsize=15_000, nproc=14):\n",
    "    \"\"\"\n",
    "    Process a list of map files to load or compute expected maps.\n",
    "\n",
    "    Parameters:\n",
    "    - files: list of str, names of the map files.\n",
    "    - path_to_maps: str, path to the directory containing map files.\n",
    "    - path_to_maps_expected: str, path to the directory for storing expected map pickles.\n",
    "    - hg38_arms: DataFrame, view dataframe for cooler.\n",
    "    - binsize: int, bin size for cooler.\n",
    "    - nproc: int, number of processes to use.\n",
    "\n",
    "    Returns:\n",
    "    - expected_maps: dict, map of filenames to their expected map DataFrames.\n",
    "    \"\"\"\n",
    "    expected_maps = {}\n",
    "    for map_name in files:\n",
    "        logging.info(f\"Processing map: {map_name}\")\n",
    "        try:\n",
    "            expected_maps[map_name] = load_or_compute_expected_map(\n",
    "                map_name, path_to_maps, path_to_maps_expected, hg38_arms, binsize, nproc\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process {map_name}: {e}\")\n",
    "    \n",
    "    return expected_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7ac2a3e-c850-43a3-bfa1-781508429514",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_maps = os.getenv('PATH_TO_MAPS')\n",
    "path_to_maps_expected = os.getenv('PATH_TO_EXPECTED_MAPS')\n",
    "path_to_custom_kernels = os.getenv('PATH_TO_CUSTOM_KERNELS')\n",
    "\n",
    "number_of_files = 23\n",
    "hg38_arms = get_chromosomes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8e37a95-38e4-422b-a39d-1809a3c0a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ballerino2022_NES_5kb.drop_diag.5kb.mcool.sampled_exact.mcool\n",
      "Lu2020_iPSC_NeuNplus.sampled_exact.mcool\n",
      "Rahman2023_FetalBrain.drop_diag.5kb.mcool.sampled_exact.mcool\n",
      "Zaghi2023_iPSC_NeuNplus.sampled_exact.mcool\n",
      "Hu2021_NeuNplus.sampled_exact.mcool\n",
      "Ballerino2022_iPSC_NeuNplus.sampled_exact.mcool\n",
      "Rahman2023_NeuNplus.sampled_exact.mcool\n",
      "Lu2020_iPSC_5kb.drop_diag.5kb.mcool.sampled_exact.mcool\n",
      "Our_data_iPSC_NeuNplus.drop_diag.5kb.mcool.sampled_exact.mcool\n",
      "Heffel_infant.3056_cells.5kb.drop_diag.5kb.sampled_exact.mcool\n",
      "Wu2021_iPSC_NeuNplus.sampled_exact.mcool\n",
      "Tian2023_NeuNplus.EN_IN.29_42_58_years.2000_cells.sampled_exact.mcool\n",
      "Ballerino2022_NPC_5kb.drop_diag.5kb.mcool.sampled_exact.mcool\n",
      "Heffel_3T.3056_cells.5kb.drop_diag.5kb.sampled_exact.mcool\n",
      "Heffel_2T.3056_cells.5kb.drop_diag.5kb.sampled_exact.mcool\n",
      "Rajarajan_iPSC_NeuNplus.sampled_exact.mcool\n",
      "Pletenev2024_NeuNplus.sampled_exact.mcool\n",
      "Rahman2023_iPSC_NeuNplus_CRISPRi_Scrambled_A_DpnII-HinfI.sampled_exact.mcool\n",
      "Rajarajan_iPSC_Glia_5kb.drop_diag.5kb.mcool.sampled_exact.mcool\n",
      "Rajarajan_NPC_5kb.drop_diag.5kb.mcool.sampled_exact.mcool\n",
      "Li2022_iPSC_NeuNplus.sampled_exact.mcool\n",
      "Zaghi2023_NPC_5kb.drop_diag.5kb.mcool.sampled_exact.mcool\n",
      "Heffel_adult.3056_cells.5kb.drop_diag.5kb.sampled_exact.mcool\n"
     ]
    }
   ],
   "source": [
    "files = load_maps(path_to_maps)\n",
    "assert len(files) == number_of_files\n",
    "expected_maps = process_maps(files, path_to_maps, path_to_maps_expected, hg38_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181221a-8421-40e3-96ba-b93fdb2d20fd",
   "metadata": {},
   "source": [
    "# 2. Define custom kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a47531ac-3023-47d7-a110-d7fe11d2c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kernels():\n",
    "    kernels_narrow = np.load(f'{path_to_custom_kernels}/kernels_narrow.npy' ,allow_pickle=True)\n",
    "    kernels_wide = np.load(f'{path_to_custom_kernels}/kernels_wide.npy' ,allow_pickle=True)\n",
    "    kernels_super_wide = np.load(f'{path_to_custom_kernels}/kernels_super_wide.npy' ,allow_pickle=True)\n",
    "    \n",
    "    kernels_narrow = dict(enumerate(kernels_narrow.flatten()))[0]\n",
    "    kernels_wide = dict(enumerate(kernels_wide.flatten()))[0]\n",
    "    kernels_super_wide = dict(enumerate(kernels_super_wide.flatten()))[0]\n",
    "    \n",
    "    return kernels_narrow, kernels_wide, kernels_super_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2095e8fc-94e4-4d18-95d5-668d3c4aada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_narrow, kernels_wide, kernels_super_wide = load_kernels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05a4bc-fb26-4add-b005-50874b0f7d35",
   "metadata": {},
   "source": [
    "# 3. Call chromatin loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0823a301-2ce8-4ab5-b053-31d1c004a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_loci_separation_set = 12000000\n",
    "def get_dots_standard(clr, exp, fdr, nans):\n",
    "    dots = cooltools.dots(\n",
    "        clr,\n",
    "        expected=exp,\n",
    "        view_df=hg38_arms,   \n",
    "        max_loci_separation = max_loci_separation_set,\n",
    "        max_nans_tolerated=nans,  \n",
    "        lambda_bin_fdr=fdr,\n",
    "        clustering_radius=binsize*2.5,\n",
    "        tile_size=7_000_000,   \n",
    "        nproc=14,\n",
    "    )\n",
    "    \n",
    "    return dots\n",
    "\n",
    "def get_dots_customkernel(clr, exp,  fdr, nans):\n",
    "    dots_customkernel = cooltools.dots(\n",
    "    clr,\n",
    "    expected=exp,\n",
    "    view_df=hg38_arms,   \n",
    "    max_loci_separation=max_loci_separation_set,\n",
    "    max_nans_tolerated=nans,  \n",
    "    kernels = kernels_wide,\n",
    "    clustering_radius=binsize*2.5,\n",
    "    lambda_bin_fdr=fdr,\n",
    "    tile_size=7_000_000,\n",
    "    nproc=14,\n",
    ")\n",
    "    return dots_customkernel\n",
    "\n",
    "\n",
    "def get_dots_customkernelCircle(clr, exp, fdr, nans):\n",
    "    dots_customkernelCircle = cooltools.dots(\n",
    "    clr,\n",
    "    expected=exp,\n",
    "    view_df=hg38_arms,   \n",
    "    max_loci_separation=max_loci_separation_set,\n",
    "    max_nans_tolerated=nans,  \n",
    "    kernels = kernels_super_wide,\n",
    "    clustering_radius=binsize*2.5,\n",
    "    lambda_bin_fdr=fdr,\n",
    "    tile_size=7_000_000,\n",
    "    nproc=14,\n",
    ")\n",
    "    return dots_customkernelCircle\n",
    "\n",
    "\n",
    "def get_dots_customkernelSmall(clr, exp, fdr, nans):\n",
    "    dots_customkernelCircle = cooltools.dots(\n",
    "    clr,\n",
    "    expected=exp,\n",
    "    view_df=hg38_arms,   \n",
    "    max_loci_separation=max_loci_separation_set,\n",
    "    max_nans_tolerated=nans,  \n",
    "    kernels = kernels_narrow,\n",
    "    clustering_radius=binsize*2.5,\n",
    "    lambda_bin_fdr=fdr,\n",
    "    tile_size=7_000_000,\n",
    "    nproc=14,\n",
    ")\n",
    "    return dots_customkernelCircle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d20b3dbd-3237-4ae9-884c-e4bb690c7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(dots_clr_HCplus_10_merged_customkernelExtreme):\n",
    "    df = dots_clr_HCplus_10_merged_customkernelExtreme.iloc[:,:6]\n",
    "    df[\"num\"] = [i for i in range(df.shape[0])]\n",
    "    return df\n",
    "\n",
    "def process_pair_to_pair(source_df, target_df, slope):\n",
    "    result = pbt.from_dataframe(source_df).pair_to_pair(pbt.from_dataframe(target_df), slop=slope)\n",
    "    if os.path.getsize(result.fn) > 0:\n",
    "        non_unique_df = pd.read_table(result.fn, header=None)\n",
    "        unique_df = source_df[~source_df[\"num\"].isin(non_unique_df[6])]\n",
    "        assert unique_df[\"num\"].nunique() == source_df[\"num\"].nunique() - non_unique_df[6].nunique(), \"Uniqueness criteria failed\"\n",
    "        return unique_df\n",
    "    else:\n",
    "        return source_df.copy()\n",
    "\n",
    "def make1_unique(df_customkernel, df_init, slope_factor):\n",
    "    unique2_to_1 = process_pair_to_pair(df_customkernel, df_init, slope_factor)\n",
    "    return unique2_to_1\n",
    "        \n",
    "def make2_unique(df_customkernelExtreme, df_init, df_customkernel, slope_factor):\n",
    "    unique3_to_2 = process_pair_to_pair(df_customkernelExtreme, df_customkernel, slope_factor)\n",
    "    unique3_to_1 = process_pair_to_pair(unique3_to_2, df_init, slope_factor)\n",
    "    return unique3_to_1\n",
    "    \n",
    "def make3_unique(df_customkernelCircle, df_customkernelExtreme, df_customkernel, df_init, slope_factor):\n",
    "    unique4_to_1 = process_pair_to_pair(df_customkernelCircle, df_init, slope_factor)\n",
    "    unique4_to_2 = process_pair_to_pair(unique4_to_1, df_customkernel, slope_factor)\n",
    "    unique4_to_1_2_3 = process_pair_to_pair(unique4_to_2, df_customkernelExtreme, slope_factor)\n",
    "    return unique4_to_1_2_3\n",
    " \n",
    "\n",
    "def create_unique_border(dots_clr_HCplus_10_merged, dots_clr_HCplus_10_merged_customkernel, dots_clr_HCplus_10_merged_customkernelExtreme, dots_customkernelCircle, binsize):\n",
    "    slope_factor = binsize * 2.5\n",
    "    \n",
    "    df_customkernelExtreme = prep_df(dots_clr_HCplus_10_merged_customkernelExtreme)\n",
    "    df_customkernel = prep_df(dots_clr_HCplus_10_merged_customkernel)\n",
    "    df_init = prep_df(dots_clr_HCplus_10_merged)\n",
    "    df_customkernelCircle = prep_df(dots_customkernelCircle)\n",
    "\n",
    "    unique2_to_1 = make1_unique(df_customkernel, df_init, slope_factor)\n",
    "    unique3_to_1 = make2_unique(df_customkernelExtreme, df_init, df_customkernel, slope_factor)\n",
    "    unique4_to_1_2_3 = make3_unique(df_customkernelCircle, df_customkernelExtreme, df_customkernel, df_init, slope_factor)\n",
    "\n",
    "    df_init['kernel'] = 'standard'\n",
    "    unique2_to_1['kernel'] = 'wide'\n",
    "    unique3_to_1['kernel'] = 'super_wide'\n",
    "    unique4_to_1_2_3['kernel'] = 'narrow'\n",
    "\n",
    "    union2 = pd.concat([df_init, unique3_to_1, unique2_to_1, unique4_to_1_2_3]).reset_index(drop=True)\n",
    "    union2[\"num\"] = list(range(union2.shape[0]))\n",
    "\n",
    "    return union2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01a6cf11-227d-4657-86d6-d80bc6a2a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_loops(file, prep_maps, binsize=10000, fdr=0.16, nans = 5, temp_dir=\"./loops_data/loops_temp_files/\", final_dir=\"./loops_data/loops_final_files\"):\n",
    "    \n",
    "    exp = prep_maps[file]\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "    clr = cooler.Cooler(f'{path_to_maps}/{file}::/resolutions/{binsize}')\n",
    "    name_exp = (\"_\").join(file.split('.')[:-1])\n",
    "    print(file, name_exp)\n",
    "\n",
    "    dots = get_dots_standard(clr, exp, fdr, nans)\n",
    "    print(dots.shape[0])\n",
    "    dots.to_feather(os.path.join(temp_dir, f\"{name_exp}_dots_regular_{max_loci_separation_set}maxloci_{fdr}fdr_{binsize}res_small_NaN{nans}.feather\"))\n",
    "\n",
    "    # Custom kernel dots\n",
    "    dot_custom = get_dots_customkernel(clr, exp, fdr, nans)\n",
    "    print(dot_custom.shape[0])\n",
    "    dot_custom.to_feather(os.path.join(temp_dir, f\"{name_exp}_dots_custom_{max_loci_separation_set}maxloci_{fdr}fdr_{binsize}res_small_NaN{nans}.feather\"))\n",
    "\n",
    "    # Custom kernel circle dots\n",
    "    dots_customkernelCircle = get_dots_customkernelCircle(clr, exp, fdr, nans)\n",
    "    print(dots_customkernelCircle.shape[0])\n",
    "    dots_customkernelCircle.to_feather(os.path.join(temp_dir, f\"{name_exp}_dots_customkernelCircle_{max_loci_separation_set}maxloci_{fdr}fdr_{binsize}res_small_NaN{nans}.feather\"))\n",
    "\n",
    "    # Small custom kernel dots\n",
    "    dot_custom_small = get_dots_customkernelSmall(clr, exp, fdr, nans)\n",
    "    print(dot_custom_small.shape[0])\n",
    "    dot_custom_small.to_feather(os.path.join(temp_dir, f\"{name_exp}_dots_small_{max_loci_separation_set}maxloci_{fdr}fdr_{binsize}res_small_NaN{nans}.feather\"))\n",
    "\n",
    "    # Create unique border dots\n",
    "    dots_final = create_unique_border(dots, dot_custom, dots_customkernelCircle, dot_custom_small, binsize)\n",
    "    dots_final.to_feather(os.path.join(temp_dir, f\"{name_exp}_dots_final_{max_loci_separation_set}maxloci_{fdr}fdr_{binsize}res_small_NaN{nans}.feather\"))\n",
    "    dots_final.to_csv(os.path.join(final_dir, f\"{name_exp}_dots_final_{max_loci_separation_set}maxloci_{fdr}fdr_{binsize}res_small_NaN{nans}_final.bed\"), sep=\"\\t\", header=None, index=False)\n",
    "    \n",
    "    return dots_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f1fd2cf-cd27-4203-8ddd-3a53f7d84c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize = 15_000\n",
    "fdr = 0.13\n",
    "binsize=15000\n",
    "\n",
    "assert set(expected_maps.keys()) == set(files)\n",
    "for file in files:\n",
    "    final_loops = get_all_loops(file, expected_maps, binsize = binsize, fdr = fdr)\n",
    "    name_exp = (\"_\").join(file.split('.')[:-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
